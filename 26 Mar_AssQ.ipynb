{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345b960f-12d4-42ab-8ec5-dc0a8cb40f36",
   "metadata": {},
   "source": [
    "Regression-1\n",
    "\n",
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec029512-8cea-4eaa-a4d9-71bf7d7270b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36936851-9b58-4559-a343-2bd615b3854e",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "**Simple Linear Regression**:\n",
    "- **Definition**: Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (outcome). The relationship is modeled as a straight line, represented by the equation:\n",
    "  \n",
    "  $ \\ [\\\n",
    "  y \\ = \\beta_0 \\ +\\ \\beta_1x \\ +\\ \\epsilon\n",
    "  \\ ]$\n",
    "  \n",
    "  where:\n",
    "  - $ \\ ( y \\ )$ is the dependent variable.\n",
    "  - $ \\ ( x \\ )$ is the independent variable.\n",
    "  - $ \\ ( \\beta_0 \\ )$ is the intercept (the value of $ \\ ( y \\ )$ when $ \\ ( x = 0 \\ ))$.\n",
    "  - $ \\ ( \\beta_1 \\ )$ is the slope (the change in $ \\ ( y \\ )$ for a one-unit change in $ \\ ( x \\ ))$.\n",
    "  - $ \\ ( \\epsilon \\ ) $is the error term (the difference between the observed and predicted values of $ \\ ( y \\ ))$.\n",
    "\n",
    "- **Example**: Suppose you want to predict a person's weight based on their height. Here, height is the independent variable $ \\ ( x \\ )$ , and weight is the dependent variable $ \\ ( y \\ )$. You might model the relationship between height and weight as:\n",
    "  \n",
    "  $ \\ [\\\n",
    "  \\text{Weight} = \\beta_0 + \\beta_1 \\times \\text{Height} + \\epsilon\n",
    "  \\ ]$ \n",
    "\n",
    "**Multiple Linear Regression**:\n",
    "- **Definition**: Multiple linear regression is an extension of simple linear regression where more than one independent variable is used to predict the dependent variable. The relationship is still modeled as a linear equation, but with multiple predictors:\n",
    "  \n",
    "  $ \\ [\\\n",
    "  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\n",
    "  \\ ]$ \n",
    "  \n",
    "  where: \n",
    "  - $ \\ ( y \\ ) $ is the dependent variable.\n",
    "  - $ \\ ( x_1, x_2, \\ldots, x_n \\ )$  are the independent variables.\n",
    "  - $ \\ ( \\beta_0 \\ ) $ is the intercept.\n",
    "  - $ \\ ( \\beta_1, \\beta_2, \\ldots, \\beta_n \\ )$  are the coefficients representing the change in \\( y \\) for a one-unit change in the respective $ \\ ( x \\ )$  variable.\n",
    "  - $ \\ ( \\epsilon \\ )$  is the error term.\n",
    "\n",
    "- **Example**: Suppose you want to predict a person's weight based on their height, age, and gender. Here, height, age, and gender are the independent variables, and weight is the dependent variable. The multiple linear regression model would be:\n",
    "\n",
    "  $ \\ [\\\n",
    "  \\text{Weight} = \\beta_0 + \\beta_1 \\times \\text{Height} + \\beta_2 \\times \\text{Age} + \\beta_3 \\times \\text{Gender} + \\epsilon\n",
    "  \\ ]$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dda54e9-b1a0-4ace-9500-a401c57a0d6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1dcf72f-11eb-4c97-adf4-4ebd70dac45b",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Linear regression relies on several key assumptions:\n",
    "\n",
    "1. **Linearity**:\n",
    "   - **Assumption**: The relationship between the independent and dependent variables is linear.\n",
    "   - **How to Check**: \n",
    "     - Plot the observed values against the predicted values (a scatter plot) and look for a linear pattern.\n",
    "     - Residual plots should show no patterns; the residuals should be randomly scattered around zero.\n",
    "\n",
    "2. **Independence**:\n",
    "   - **Assumption**: The observations are independent of each other.\n",
    "   - **How to Check**:\n",
    "     - This is often more about the study design. Ensure that data points are collected independently.\n",
    "     - In time series data, check for autocorrelation using tools like the Durbin-Watson statistic.\n",
    "\n",
    "3. **Homoscedasticity**:\n",
    "   - **Assumption**: The variance of the errors (residuals) is constant across all levels of the independent variables.\n",
    "   - **How to Check**:\n",
    "     - Plot residuals against predicted values. The spread of residuals should be roughly the same across all levels of the predicted values (no funnel shape).\n",
    "     - Use statistical tests like Breusch-Pagan test or Whiteâ€™s test.\n",
    "\n",
    "4. **Normality of Residuals**:\n",
    "   - **Assumption**: The residuals are normally distributed.\n",
    "   - **How to Check**:\n",
    "     - Create a histogram or Q-Q plot of the residuals to assess normality.\n",
    "     - Perform a statistical test like the Shapiro-Wilk test for normality.\n",
    "\n",
    "5. **No Multicollinearity** (for multiple linear regression):\n",
    "   - **Assumption**: Independent variables should not be highly correlated with each other.\n",
    "   - **How to Check**:\n",
    "     - Calculate the Variance Inflation Factor (VIF). A VIF value greater than 10 indicates high multicollinearity.\n",
    "     - Compute pairwise correlation coefficients between the independent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c8d303a-0d07-4dce-b0cf-f995d33cf4c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6616268-13a9-4f0e-97e0-fa820aec1b60",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "- **Intercept $( \\ (\\beta_0 \\ ))$**:\n",
    "  - The intercept is the value of the dependent variable $ \\ ( y \\ )$ when all independent variables $ \\ ( x \\ )$ are zero. It represents the baseline level of $ \\ ( y \\ )$ in the absence of any effects from the predictors.\n",
    "  \n",
    "  - **Example**: Suppose you are modeling the relationship between years of experience (independent variable) and salary (dependent variable) in a company:\n",
    "    $$ \\ [\n",
    "    \\text{Salary} = \\beta_0 + \\beta_1 \\times \\text{Years of Experience} + \\epsilon\n",
    "    \\ ]$$\n",
    "    If the intercept $( \\ (\\beta_0 \\ ))$ is $30,000, this means that even with zero years of experience, the expected starting salary is $30,000.\n",
    "\n",
    "- **Slope $( \\ (\\beta_1 \\ ))$**:\n",
    "  - The slope represents the change in the dependent variable $ \\ ( y \\ )$ for a one-unit change in the independent variable $ \\ ( x \\ )$.\n",
    "  \n",
    "  - **Example**: In the same salary prediction model, if the slope $( \\ (\\beta_1 \\ ))$ is $2,000, this indicates that for every additional year of experience, the salary is expected to increase by $2,000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06b7df70-8f11-48ba-964f-be1fb6749b81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b1a4da9-b77d-44a5-bb02-11666279b911",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "- **Concept**:\n",
    "  - Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent (downward slope) of the function. It is commonly used to find the minimum of a cost function in machine learning, particularly in linear regression and neural networks.\n",
    "  \n",
    "- **How It Works**:\n",
    "  - **Initialization**: Start with an initial guess for the model parameters (e.g., coefficients in linear regression).\n",
    "  - **Compute the Gradient**: Calculate the gradient of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase of the function.\n",
    "  - **Update Parameters**: Update the parameters by moving in the opposite direction of the gradient. The size of the step is determined by the learning rate $( \\ (\\alpha\\ ))$ .\n",
    "    $$\\ [\n",
    "    \\ theta = \\theta - \\alpha \\cdot \\frac{ \\partial J( \\theta ) } { \\partial \\theta}\n",
    "    \\ ]$$\n",
    "    where:\n",
    "    - $\\ ( \\$lpha \\ ) $is the learning rate.\n",
    "    - $\\ ( J( \\ theta) \\ ) $is the cost function.\n",
    "  - **Iterate**: Repeat the process until the parameters converge to the minimum of the cost function.\n",
    "\n",
    "- **Use in Machine Learning**:\n",
    "  - Gradient descent is used to minimize the cost function in linear regression, where the cost function represents the difference between the predicted and actual values. By iteratively updating the parameters, gradient descent finds the best-fitting line.\n",
    "  - In more complex models like neural networks, gradient descent is used to optimize weights and biases by minimizing the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c1897a7-185a-41f6-9e83-95a3550b7616",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93a89432-f518-497d-a4d0-11cfc5f0b97c",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "- **Multiple Linear Regression**:\n",
    "  - Multiple linear regression models the relationship between a dependent variable and multiple independent variables. The equation is:\n",
    "    $$ \\ [\n",
    "    y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\n",
    "    \\ ]$$ \n",
    "    where:\n",
    "    - $ \\ ( y \\ )$  is the dependent variable.\n",
    "    - $ \\ ( x_1, x_2, \\ldots, x_n \\ )$  are the independent variables.\n",
    "    - $ \\ ( \\beta_0 \\ )$ is the intercept.\n",
    "    - $ \\ ( \\beta_1, \\beta_2, \\ldots, \\beta_n \\ )$  are the coefficients.\n",
    "    - $ \\ ( \\epsilon \\ )$  is the error term.\n",
    "\n",
    "- **Difference from Simple Linear Regression**:\n",
    "  - **Number of Predictors**: Simple linear regression involves one independent variable, while multiple linear regression involves two or more independent variables.\n",
    "  - **Complexity**: Multiple linear regression is more complex as it takes into account the effect of multiple predictors, potentially leading to interactions and multicollinearity.\n",
    "  - **Interpretation**: In multiple linear regression, each coefficient represents the effect of a particular independent variable on the dependent variable, holding all other variables constant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fe721c2-c362-4c5d-8082-c64cfe02695c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e685bef2-7ccf-4e42-ab40-0692bae232c0",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "- **Multicollinearity**:\n",
    "  - Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, meaning they provide redundant information about the dependent variable. This can lead to unreliable estimates of the coefficients, inflated standard errors, and difficulties in assessing the individual impact of each predictor.\n",
    "\n",
    "- **Detection**:\n",
    "  - **Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c77ec-9fdc-4a07-a2ff-ee311275c8e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c698a5-83c8-4bd4-a6c5-85490a71c849",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
